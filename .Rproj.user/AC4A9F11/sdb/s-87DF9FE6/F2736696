{
    "collab_server" : "",
    "contents" : "#' Forward feature selection\n#' @description A simple forward feature selection algorithm\n#' @param predictors see \\code{\\link{train}}\n#' @param response see \\code{\\link{train}}\n#' @param method see \\code{\\link{train}}\n#' @param metric see \\code{\\link{train}}\n#' @param maximize see \\code{\\link{train}}\n#' @param withinSD Logical Models are only selected if they are better than the\n#' currently best models Standard error\n#' @param trControl see \\code{\\link{train}}\n#' @param tuneLength see \\code{\\link{train}}\n#' @param tuneGrid see \\code{\\link{train}}\n#' @param seed A random number\n#' @param runParallel Logical\n#' @param numeric Number of cores for parallel processing\n#' @param ... arguments passed to the classification or regression routine\n#' (such as randomForest). Errors will occur if values for tuning parameters are\n#' passed here.\n#' @return A list of class train. Beside of the usual train contentm\n#' the object contains the vector \"selectedvars\" and \"selectedvars_perf\"\n#' that give the order of the variables selected as well as their corresponding\n#' performance (starting from the first two variables)\n#' @details Models with two predictors are first trained using all possible\n#' pairs of predictor variables. The best model of these initial models is kept.\n#' On the basis of this best model the predictor variables are iteratively\n#' increased and each of the remaining variables is tested for its improvement\n#' of the currently best model. The process stops if none of the remaining\n#' variables increases the model performance when added to the current best model.\n#'\n#' The internal cross validation can be run in parallel. See information\n#' on parallel processing of carets train functions for details.\n#'\n#' Using withinSE will favour models with less variables and\n#' probably shorten the calculation time\n#'\n#' @note This validation is particulary suitable for\n#' leave-location-out cross validations where variable selection\n#' MUST be based on the performance of the model on the hold out station.\n\n#' @author Hanna Meyer\n#' @seealso \\code{\\link{train}},\n#' \\code{\\link{trainControl}},\\code{\\link{CreateSpacetimeFolds}}\n#' @examples\n#'  \\dontrun{\n#' data(iris)\n#' ffsmodel <- ffs(iris[,1:4],iris$Species)\n#' ffsmodel$selectedvars\n#' ffsmodel$selectedvars_perf\n#' }\n#'\n#' # or perform model with target-oriented validation (LLO CV)\n#' #the example is taken from the GSIF package and is described\n#' #in Gasch et al. (2015). Using target-oriented validation, the reference is\n#' #Meyer et al. (in prep). Due to high computation time needed, only a\n#' #small and thus not robust example is shown here.\n#'\n#' dat <- get(load(system.file(\"extdata\",\"Cookfarm.RData\",package=\"CAST\")))\n#' trainDat <- dat[createDataPartition(dat$VW, p = 0.001,list=FALSE),]\n#' indices <- CreateSpacetimeFolds(trainDat,spacevar = \"SOURCEID\")\n#' predictors <- c(\"DEM\",\"TWI\",\"NDRE.M\",\"Bt\",\"BLD\",\"PHI\",\"Precip_cum\",\"cdayt\")\n#' ffsmodel <- ffs(trainDat[,predictors],trainDat$VW,method=\"rf\",\n#' trControl=trainControl(method=\"cv\",index=indices$index,indexOut=indices$indexOut),\n#' tuneLength=1)\n#'\n#' @export ffs\n#' @aliases ffs\n\nffs <- function (predictors,\n                 response,\n                 method = \"rf\",\n                 metric = ifelse(is.factor(response), \"Accuracy\", \"RMSE\"),\n                 maximize = ifelse(metric == \"RMSE\", FALSE, TRUE),\n                 withinSD = FALSE,\n                 trControl = trainControl(),\n                 tuneLength = 3,\n                 tuneGrid = NULL,\n                 seed = sample(1:1000, 1),\n                 runParallel = FALSE,\n                 cores = detectCores(),\n                 ...){\n  require(caret)\n  if(runParallel){\n    require(doParallel)\n    cl <- makeCluster(cores)\n    registerDoParallel(cl)\n  }\n  n <- length(names(predictors))\n  acc <- 0\n  if(maximize) evalfunc <- function(x){max(x,na.rm=T)}\n  if(!maximize) evalfunc <- function(x){min(x,na.rm=T)}\n  isBetter <- function (actmodelperf,bestmodelperf,\n                        bestmodelperfSD=NULL,\n                        maximization=FALSE,\n                        withinSE=FALSE){\n    if(withinSE){\n      result <- ifelse (!maximization, actmodelperf < bestmodelperf-bestmodelperfSD,\n                        actmodelperf > bestmodelperf+bestmodelperfSD)\n    }else{\n      result <- ifelse (!maximization, actmodelperf < bestmodelperf,\n                        actmodelperf > bestmodelperf)\n    }\n    return(result)\n  }\n  #### chose initial best model from all combinations of two variables\n  twogrid <- t(data.frame(combn(names(predictors),2)))\n  for (i in 1:nrow(twogrid)){\n    set.seed(seed)\n    model <- train(predictors[,twogrid[i,]],\n                   response,\n                   method=method,\n                   trControl=trControl,\n                   tuneLength = tuneLength,\n                   tuneGrid = tuneGrid)\n    ### compare the model with the currently best model\n    actmodelperf <- evalfunc(model$results[,names(model$results)==metric])\n    if(withinSD){\n      actmodelperfSD <- Rsenal::se(\n        sapply(unique(model$resample$Resample),\n               FUN=function(x){mean(model$resample[model$resample$Resample==x,\n                                                   metric])}))\n\n    }\n    if (i == 1){\n      bestmodelperf <- actmodelperf\n      if(withinSD){\n        bestmodelperfSD <- actmodelperfSD\n      }\n      bestmodel <- model\n    } else{\n      if (isBetter(actmodelperf,bestmodelperf,maximization=maximize,withinSE=FALSE)){\n        bestmodelperf <- actmodelperf\n        if(withinSD){\n          bestmodelperfSD <- actmodelperfSD\n        }\n        bestmodel <- model\n      }\n    }\n    acc <- acc+1\n    print(paste0(\"maxmimum number of models that still need to be trained: \",\n                 2*(n-1)^2/2-acc))\n  }\n  #### increase the number of predictors by one (try all combinations)\n  #and test if model performance increases\n  selectedvars <- names(bestmodel$trainingData)[-which(\n    names(bestmodel$trainingData)==\".outcome\")]\n  if (maximize){\n    selectedvars_perf <- max(bestmodel$results[,metric])\n  } else{\n    selectedvars_perf <- min(bestmodel$results[,metric])\n  }\n  print(paste0(paste0(\"vars selected: \",paste(selectedvars, collapse = ',')),\n               \" with \",metric,\" \",round(selectedvars_perf,3)))\n  for (k in 1:(length(names(predictors))-2)){\n    startvars <- names(bestmodel$trainingData)[-which(\n      names(bestmodel$trainingData)==\".outcome\")]\n    nextvars <- names(predictors)[-which(\n      names(predictors)%in%startvars)]\n    if (length(startvars)<(k+1)){\n      message(paste0(\"Note: No increase in performance found using more than \",\n                     length(startvars), \" variables\"))\n      bestmodel$selectedvars <- selectedvars\n      bestmodel$selectedvars_perf <- selectedvars_perf[-length(selectedvars_perf)]\n      return(bestmodel)\n      break()\n    }\n    for (i in 1:length(nextvars)){\n      set.seed(seed)\n      model <- train(predictors[,c(startvars,nextvars[i])],\n                     response,\n                     method = method,\n                     trControl = trControl,\n                     tuneLength = tuneLength,\n                     tuneGrid = tuneGrid)\n      actmodelperf <- evalfunc(model$results[,names(model$results)==metric])\n      if(withinSD){\n        actmodelperfSD <- Rsenal::se(\n          sapply(unique(model$resample$Resample),\n                 FUN=function(x){mean(model$resample[model$resample$Resample==x,\n                                                     metric])}))\n      }\n      if(isBetter(actmodelperf,bestmodelperf,bestmodelperfSD,\n                  maximization=maximize,withinSE=withinSD)){\n        bestmodelperf <- actmodelperf\n        if(withinSD){\n          bestmodelperfSD <- actmodelperfSD\n        }\n        bestmodel <- model\n\n\n      }\n      acc <- acc+1\n      print(paste0(\"maxmimum number of models that still need to be trained: \",\n                   2*(n-1)^2/2-acc))\n    }\n    selectedvars <- c(selectedvars,names(bestmodel$trainingData)[-which(\n      names(bestmodel$trainingData)%in%c(\".outcome\",selectedvars))])\n    if (maximize){\n      selectedvars_perf <- c(selectedvars_perf,max(bestmodel$results[,metric]))\n      print(paste0(paste0(\"vars selected: \",paste(selectedvars, collapse = ',')),\n                   \" with \", metric,\" \",round(max(bestmodel$results[,metric]),3)))\n    }\n    if (!maximize){\n      selectedvars_perf <- c(selectedvars_perf,min(bestmodel$results[,metric]))\n      print(paste0(paste0(\"vars selected: \",paste(selectedvars, collapse = ',')),\n                   \" with \",metric,\" \",round(min(bestmodel$results[,metric]),3)))\n    }\n  }\n  if(runParallel){\n    stopCluster(cl)\n  }\n  bestmodel$selectedvars <- selectedvars\n  bestmodel$selectedvars_perf <- selectedvars_perf\n  return(bestmodel)\n}\n",
    "created" : 1498493079976.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3156534148",
    "id" : "F2736696",
    "lastKnownWriteTime" : 1498496407,
    "last_content_update" : 1498496418307,
    "path" : "~/Documents/Release/environmentalinformatics-marburg/CAST/R/ffs.R",
    "project_path" : "R/ffs.R",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}